import json
import os
import sys
from typing import Dict, List
import re
import statistics
import asyncio

from src.services.llm import structured_output, PausesSuggestionLLM, PauseCoachLLM

async def suggest_pauses_async(asr_output: dict) -> List[int]:
    """Return word indices where a brief pause *before* the word is advised.

    Parameters
    ----------
    asr_output : dict
        Whisper / Deepgram *verbose_json* transcript – must contain a top-level
        ``"words"`` list where each item has the keys ``start``, ``end`` and
        ``word``.

    Returns
    -------
    list[int]
        Zero-based indices into ``words`` that indicate recommended pause
        boundaries.  The index refers to the *following* word so callers can
        insert a break **before** that token when rendering subtitles or
        generating coaching feedback.
    """
    words = asr_output.get("words", [])
    if not words:
        return []
    
    transcript = " ".join(w["word"] for w in words)

    example_input = "My name is bond James Bond"
    example_output = "My name is bond [PAUSE] James Bond"

    prompt = (
        "You are an expert in spoken communication. Analyze this transcript "
        "and insert \"[PAUSE]\" tokens where brief pauses would improve "
        "clarity, emphasis, or natural flow. Follow these rules:\n\n"
        "1. PRESERVE all original words exactly as given\n"
        "2. ONLY insert \"[PAUSE]\" tokens – no other changes\n"
        "3. Insert pauses only at natural break points:\n"
        "   - Before important words for emphasis\n"
        "   - Between logical thought groups\n"
        "   - After conjunctions or transitional phrases\n"
        "   - Before appositives or clarifying information\n"
        "4. Never add punctuation or modify words\n"
        "5. Never insert a pause before the first word\n\n"
        f"Example Input: \"{example_input}\"\n"
        f"Example Output: \"{example_output}\"\n\n"
        "Now process this transcript:\n"
        f"\"{transcript}\"\n\n"
        "Output ONLY the modified transcript with \"[PAUSE]\" tokens. "
        "Do not include any other text or explanations."
    )
    
    try:
        result, err, _lat, _model = await structured_output(
            PausesSuggestionLLM,
            system_prompt=prompt,
            user_content="",
            temperature=0,
        )
        response = (result.modified_transcript if result else transcript).strip('"')
    except Exception:
        response = transcript.strip('"')

    return _find_pause_indices(words, response)

def _find_pause_indices(original_words: List[dict], paused_transcript: str) -> List[int]:
    """Compare *paused_transcript* with *original_words* to locate [PAUSE] tags.

    The helper performs a simple token-by-token alignment, tolerant to minor
    mismatches that can happen when the LLM accidentally drops or duplicates a
    word.  Whenever a \"[PAUSE]\" token is encountered **and** the following
    response token matches the *current* original word we record the index.  A
    pause marker that appears right at the end of the string is ignored as it
    cannot be mapped to a *following* word.
    """

    # pdb.set_trace()
    original_tokens = [w["word"] for w in original_words]
    response_tokens = paused_transcript.split()

    pause_indices: List[int] = []
    orig_idx = 0
    resp_idx = 0

    while orig_idx < len(original_tokens) and resp_idx < len(response_tokens):
        token = response_tokens[resp_idx]

        if token == "[PAUSE]":

            resp_idx += 1  # advance to next response token

            if resp_idx >= len(response_tokens):
                break  # dangling [PAUSE] at the very end – ignore

            if response_tokens[resp_idx] == original_tokens[orig_idx]:
                if orig_idx > 0:
                    pause_indices.append(orig_idx)

            continue

        if token == original_tokens[orig_idx]:
            orig_idx += 1
            resp_idx += 1
        else:
            resp_idx += 1

    seen = set()
    deduped: List[int] = []
    for idx in pause_indices:
        if idx not in seen:
            seen.add(idx)
            deduped.append(idx)

    return deduped


# ---------------------------------------------------------------------------
# Utility helpers
# ---------------------------------------------------------------------------


def _format_ts(seconds: float) -> str:
    """Convert raw seconds to "MM:SS" string for human-friendly references."""
    mins = int(seconds // 60)
    secs = int(seconds % 60)
    return f"{mins:02d}:{secs:02d}"


def _extract_pauses(words: List[dict]) -> List[dict]:
    """Return a list of pauses with useful metadata."""
    pauses: List[Dict] = []
    for i in range(len(words) - 1):
        pause_duration = words[i + 1]["start"] - words[i]["end"]
        if pause_duration <= 0:
            # overlapping words – ignore
            continue
        pauses.append(
            {
                "index": i,
                "start": words[i]["end"],
                "end": words[i + 1]["start"],
                "duration": pause_duration,
                "before_word": words[i]["word"],
                "after_word": words[i + 1]["word"],
            }
        )
    return pauses

async def coach_feedback_async(coaching_prompt: str) -> Dict:
    try:
        result, err, _lat, _model = await structured_output(
            PauseCoachLLM,
            system_prompt=coaching_prompt,
            user_content="",
            temperature=0,
        )
        if result:
            return {"actionable_feedback": result.actionable_feedback, "score": int(result.score)}
    except Exception:
        pass
    return {}

def extract_json(text: str):
    """
    Extracts and parses the first valid JSON object or array from given text

    Args:
        text (str): LLM Prompt whose response expects a json.

    Returns:
        dict or list: Parsed JSON object or array.

    Raises:
        ValueError: If no valid JSON is found.
    """
    try:
        start = min(
            (text.index('{') if '{' in text else float('inf')),
            (text.index('[') if '[' in text else float('inf'))
        )
        end = max(
            (text.rindex('}') + 1 if '}' in text else -1),
            (text.rindex(']') + 1 if ']' in text else -1)
        )
        json_str = text[start:end]

        # Remove trailing commas
        json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)

        # Escape raw newlines inside quoted strings
        def escape_newlines_in_strings(match):
            return match.group(0).replace("\n", "\\n")

        json_str = re.sub(r'"([^"\\]*(\\.[^"\\]*)*)"', escape_newlines_in_strings, json_str)
        
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        
        raise ValueError(f"Invalid JSON found: {e}")


async def analyze_pauses_async(asr_output: dict):
    """Analyse pauses and generate actionable feedback.

    Parameters
    ----------
    asr_output : dict
        The *verbose_json* output of Whisper / Deepgram.  Must contain a
        top-level ``"words"`` list with ``start``, ``end`` & ``word`` keys.
    call_llm : Callable[[str], str]
        Convenience wrapper around OpenAI chat completion that the main API
        code already provides.  Must take a *prompt* and return the raw model
        response.

    Returns
    -------
    dict
        A dictionary with the following keys:

        overview        – single-sentence summary
        details         – list of granular comments with timestamps
        distribution    – % distribution of pause types
        actionable_feedback – concise coaching paragraph generated by LLM
        score           – integer (1-5) following the explicit rubric
    """
    inp = type(asr_output), [(k,v) for k,v in asr_output.items()]
    if asr_output.get('words_timestamp',None) is not None:
        asr_output = asr_output['words_timestamp']
        
    words = asr_output.get("words", [])

    words = [w for w in words if 'start' in w and 'end' in w]

    if not words:
        return {
            "overview": f"No word-level timestamps provided – unable to analyse pauses, instead given {inp} as input",
            "details": [],
            "distribution": {},
            "actionable_feedback": "Please re-upload the audio so that word timings are included.",
            "score": 1,
        }

    pauses = _extract_pauses(words)

    try:
        recommended_pause_indices = await suggest_pauses_async(asr_output)
    except Exception:
        if os.getenv("PAUSES_DEBUG"):
            print("WARNING: analyze_pauses – recommended indices fallback", file=sys.stderr)
        recommended_pause_indices = []

    # ------------------------------------------------------------------
    # 3. Classify pauses ---------------------------------------------------
    # ------------------------------------------------------------------
    # Dynamic thresholding based on the *speaker's own* statistics --------
    # ------------------------------------------------------------------
    # A one-size-fits-all threshold (e.g. “long pause > 3 s”) implicitly
    # assumes ~120 WPM which breaks down for very quick or very slow
    # speakers.  Instead of hard-coding numbers we derive **all** pause
    # categories from the observed distribution in the current answer.
    #
    #   • rushed      – shorter than the 25-th percentile (Q1)
    #   • long        – longer than the 75-th percentile (Q3) *and* at least
    #                   1 s so that tiny datasets do not cause absurdisms.
    #   • strategic   – between 0.8×median .. 1.5×median (roughly
    #                   “noticeable but not disruptive”) *and* before an
    #                   important technical term **or** a new sentence.
    #
    # When there are fewer than 8 pauses (very short answers) the quartile
    # estimation becomes unstable.  In that case we fall back to a secondary
    # heuristic based on words-per-minute (WPM) ‑ the previous, simpler
    # implementation – so we never fully lose coverage.
    # ------------------------------------------------------------------

    pause_durations = [p["duration"] for p in pauses]

    # Helper: fallback WPM-scaled numbers ---------------------------------
    def _wpm_scaled_thresholds() -> tuple[float, float, float, float]:
        """Return (long_thr, rushed_thr, strategic_min, strategic_max)."""
        print("Using WPM-scaled thresholds")
        total_words = len(words)
        if words:
            total_time = words[-1]["end"] - words[0]["start"]
        else:
            total_time = 0.0

        if total_time <= 0:
            wpm = 120  # assume average rate
        else:
            wpm = (total_words / total_time) * 60

        scale = 120 / wpm if wpm > 0 else 1.0

        # Ensure sane defaults that hold across typical speaking rates (80‒180
        # WPM).  We intentionally keep the numbers aligned with the fixed
        # bounds used in the quartile-based branch so that callers see
        # consistent behaviour regardless of answer length.

        long_thr = 1.0 * scale if scale > 1 else 1.0  # minimum 1 s
        long_thr = min(long_thr, 3.0)  # never mark >3 s as acceptable

        rushed_thr = 0.1 * scale if scale < 1 else 0.1  # ≈100 ms @120 WPM
        rushed_thr = max(0.05, min(rushed_thr, 0.2))

        # Broaden strategic pause window – see detailed explanation further
        # below in the quartile–based branch.
        strat_min = 0.15
        strat_max = 2.5
        return long_thr, rushed_thr, strat_min, strat_max

    # Determine thresholds -------------------------------------------------
    if len(pause_durations) >= 8:
        # Use robust Tukey's five-number summary for larger datasets
        try:
            q1, q3 = statistics.quantiles(pause_durations, n=4)[0], statistics.quantiles(pause_durations, n=4)[2]
        except Exception:
            # Very unlikely, but keep the code safe
            print("WARNING: quantile computation failed", file=sys.stderr)
            q1 = q3 = None

        if q1 is not None and q3 is not None:
            # ------------------------------------------------------------------
            # Derive **robust** thresholds while keeping them within sensible
            # physiological limits.  Using the raw quartiles alone leads to
            # unrealistically small numbers for short samples (e.g. Q3 ≈ 0.7 s
            # → *anything* above 0.7 s would be flagged as a long pause).  We
            # therefore clamp the automatically derived values to a proven
            # lower / upper bound that reflects typical human speech patterns.
            # ------------------------------------------------------------------

            # 1.  Rushed – instead of the full first quartile we use **half**
            #     of Q1 which empirically separates *true* word-sandwiching
            #     (no audible gap at all) from legitimate quick pacing.  We
            #     still cap the lower bound at 20 ms to avoid classifying
            #     timestamp noise as rushed, and cap the upper bound at
            #     120 ms which is around the shortest silence most listeners
            #     reliably perceive.

            rushed_threshold = max(0.02, min(0.12, q1 * 0.5))

            # 2.  Long – a pause only becomes disruptive when it *clearly* sits
            #     outside the speaker's usual rhythm.  We therefore set the
            #     threshold to **max(1.5 s, 2×Q3)** so that isolated dramatic
            #     pauses of ~1.8 s (common in keynote-style delivery) are not
            #     penalised.  The cap of 3 s from the earlier version is
            #     retained implicitly because 1.5 s ≤ long_threshold ≤ 3 s in
            #     typical data.

            # Choose the larger of a fixed 2-second cut-off or three times the
            # 75th-percentile so that occasional dramatic pauses (~1.8 s) are
            # not marked as disruptive, while still flagging *truly* lengthy
            # gaps above ≈3 s.
            long_threshold = max(2.0, q3 * 3)

            # 3.  Strategic – a *good* pause varies greatly with speaking
            #     style and context.  Real-world recordings show useful pauses
            #     as short as ~50 ms up to a bit more than two seconds.  We
            #     therefore broaden the acceptance window so that pauses that
            #     were **explicitly** suggested by the LLM are not
            #     mis-classified just because they lie outside the narrow
            #     0.3-1.5 s range that was previously hard-coded.

            # Allow very brief emphasising hesitations (≥50 ms) and also
            # extended dramatic pauses (≤2.5 s) while still excluding
            # outliers that would almost certainly feel disruptive (>3 s).
            strategic_min = 0.15
            strategic_max = 2.5
        else:
            long_threshold, rushed_threshold, strategic_min, strategic_max = _wpm_scaled_thresholds()
    else:
        print("WARNING: very short answer – using WPM scaled thresholds", file=sys.stderr)
        long_threshold, rushed_threshold, strategic_min, strategic_max = _wpm_scaled_thresholds()

    long_pauses: List[Dict] = []
    rushed_pauses: List[Dict] = []
    strategic_pauses: List[Dict] = []
    

    for pause in pauses:
        i = pause["index"]  # index of the word *before* the pause

        # --------------------------------------------------------------
        # Determine basic categories via duration thresholds ----------
        # --------------------------------------------------------------
        # 3. Strategic – lies inside the *noticeable but not disruptive* band
        #    AND was explicitly suggested by the LLM.
        # Prioritise pauses that the LLM explicitly recommended.  In most
        # cases these will fall inside the *strategic* window.  However, when
        # the actual silence is either shorter or longer than the ideal
        # range we still want to classify it rather than silently discarding
        # the event.  Therefore we *only* short-circuit when the pause is a
        # genuine strategic one.  Otherwise we drop through to the generic
        # duration-based checks so that an overly long recommended pause is
        # still reported as "long" and an extremely brief one as "rushed".

        # 1. Strategic pause – every mid-length silence is potentially helpful.
        if strategic_min <= pause["duration"] <= strategic_max:
            strategic_pauses.append(pause)
            continue

        # 2. Long pause – noticeably disruptive
        if pause["duration"] > long_threshold:
            long_pauses.append(pause)
            continue

        # 3. Rushed – extremely short transitions that make the delivery feel
        #    breathless.  We still exempt explicitly recommended indices to
        #    avoid double-penalising intentional, very brief emphasis cues.
        if pause["duration"] < rushed_threshold:
            if (i + 1) not in recommended_pause_indices:
                rushed_pauses.append(pause)
            continue
        

    # ------------------------------------------------------------------
    # 4. Build deterministic feedback (examples, distribution)  -----------
    feedback: Dict = {"overview": "", "details": [], "distribution": {}}

    templates = {
        "long": (
            long_pauses,
            "⚠️ Long pause ({duration:.1f}s) after '{before_word}' at {timestamp}: consider a short linking phrase to keep the flow.",
            f"{len(long_pauses)} overly long pauses (> {long_threshold:.2f}s)",
        ),
        "rushed": (
            rushed_pauses,
            "⚠️ Rushed transition ({duration:.1f}s) between '{before_word}' → '{after_word}' at {timestamp}: add a tiny pause so listeners can follow.",
            f"{len(rushed_pauses)} rushed transitions (< {rushed_threshold:.2f}s)",
        ),
        "strategic": (
            strategic_pauses,
            "✅ Good pause ({duration:.1f}s) before '{after_word}' at {timestamp}: nice emphasis.",
            f"{len(strategic_pauses)} well-placed strategic pauses",
        ),
    }

    for kind, (examples, template, summary) in templates.items():
        if not examples:
            continue
        # add up to two illustrative examples
        for ex in examples[:2]:
            # Provide human-readable timestamp at the start of the pause
            ex_with_time = {**ex, "timestamp": _format_ts(ex["start"])}
            feedback["details"].append(template.format(**ex_with_time))
        feedback["overview"] += (", " if feedback["overview"] else "") + summary

    total_pauses = len(pauses)
    if total_pauses:
        feedback["distribution"] = {
            "long": f"{len(long_pauses) / total_pauses:.1%}",
            "rushed": f"{len(rushed_pauses) / total_pauses:.1%}",
            "strategic": f"{len(strategic_pauses) / total_pauses:.1%}",
            "normal": f"{(total_pauses - len(long_pauses) - len(rushed_pauses) - len(strategic_pauses)) / total_pauses:.1%}",
        }

    # ------------------------------------------------------------------
    # Additional quality signals ---------------------------------------
    # ------------------------------------------------------------------
    strategic_mean_duration = 0.0
    if strategic_pauses:
        strategic_mean_duration = sum(p["duration"] for p in strategic_pauses) / len(strategic_pauses)

    if not feedback["overview"]:
        feedback["overview"] = "Good pause management overall"
        feedback["details"].append("✅ Pause patterns support clear communication")

    # ------------------------------------------------------------------
    # 5. Ask LLM for actionable feedback + score --------------------------
    # ------------------------------------------------------------------
    # ------------------------------------------------------------------
    # Updated, more forgiving rubric ------------------------------------
    # ------------------------------------------------------------------
    #  The original thresholds were found to penalise natural-sounding,
    #  studio-quality samples that contain intentional dramatic pauses or
    #  micro-pauses produced by alignment jitter.  We now align the
    #  categories closer to real-world data:
    #
    #    •  “Long” pauses become disruptive only when they make up >10 % of
    #       all silences (instead of 5 %).
    #    •  “Rushed” transitions start to hurt intelligibility once they
    #       exceed ~15 % of pauses (instead of 10 %).
    #    •  Helpful “strategic” pauses are rewarded at a lower threshold of
    #       8 % so that concise answers can still hit the top score.
    #
    #  These numbers were calibrated against the curated sample set in
    #  pauses_input_samples where *eleven_pause_x.json* serves as a reference
    #  for near-ideal pacing.
    # ------------------------------------------------------------------

    rubric = (
        "### Pause Management Scoring Rubric (1‒5)\n"
        "5 – Excellent: strategic pauses ≥20 % **and** rushed ≤10 % **and** long ≤10 %.\n"
        "4 – Good: strategic 10-<20 % with rushed ≤20 % and long ≤15 %.\n"
        "3 – Fair: strategic 5-<10 % **or** (rushed 20-35 % / long 15-20 %).\n"
        "2 – Poor: strategic <5 % **or** >20 % long **or** >35 % rushed.\n"
        "1 – Very poor: long pauses >30 % **or** rushed pauses >50 %.\n"
    )

    stats_for_prompt = (
        f"Long pauses : {feedback['distribution'].get('long', '0%')}\n"
        f"Rushed pauses: {feedback['distribution'].get('rushed', '0%')}\n"
        f"Strategic    : {feedback['distribution'].get('strategic', '0%')}\n"
    )

    coaching_prompt = (
        "You are an interview communication coach. Use **simple, everyday language** "
        "(aim for a grade-6 reading level). Your task:\n"
        "1. Evaluate the speaker's pauses based on the stats below.\n"
        "2. Give **actionable** advice. Cite the exact word(s) and the timestamp you are referring to in parentheses so users know where to improve "
        "(e.g., after 'model' 01:22).\n"
        "3. Assign a holistic score from 1-5 following the rubric.\n\n"
        f"{rubric}\n"
        "---\n"
        "STATISTICS\n"
        f"{stats_for_prompt}\n"
        "EXAMPLE ISSUES\n"
        + "\n".join(feedback["details"]) + "\n---\n"
        "Return a JSON object with exactly these keys: 'actionable_feedback' (string) and 'score' (integer)."
    )

    # ------------------------------------------------------------------
    # 4.5  Prepare baseline heuristic score -----------------------------
    # ------------------------------------------------------------------
    actionable_feedback = "Could not generate feedback – LLM error."
    # ``score`` will be set *after* the heuristic has been computed so that
    # the baseline always has a valid value even when the LLM call fails.

    # Pre-compute a *deterministic* score so we can later reconcile any LLM
    # response with the strict rubric.  This guarantees consistent results
    # across different model versions while still allowing the large model to
    # craft human-friendly feedback text.
    long_pct_val = float(feedback["distribution"].get("long", "0%")[:-1])
    rushed_pct_val = float(feedback["distribution"].get("rushed", "0%")[:-1])
    strategic_pct_val = float(feedback["distribution"].get("strategic", "0%")[:-1])

    heuristic_score = 3  # neutral default
    # Helper to avoid division by zero
    def _safe_ratio(a: float, b: float) -> float:
        return a / b if b > 0 else float("inf")

    strategic_rushed_ratio = _safe_ratio(strategic_pct_val, rushed_pct_val)

    # Tier-1 – Excellent
    if (
        strategic_pct_val >= 20
        and rushed_pct_val <= 10
        and long_pct_val <= 10
        and strategic_mean_duration >= 0.25
    ):
        heuristic_score = 5

    # Tier-2 – Good
    elif (
        strategic_pct_val >= 10
        and strategic_rushed_ratio >= 2.5
        and rushed_pct_val <= 20
        and long_pct_val <= 15
        and strategic_mean_duration >= 0.2
    ):
        heuristic_score = 4

    # Tier-3 – Fair
    elif (
        strategic_pct_val >= 5 or strategic_mean_duration >= 0.15
    ) and (long_pct_val <= 20 and rushed_pct_val <= 35):
        heuristic_score = 3

    # Tier-4 / Tier-5 – Poor / Very poor
    else:
        heuristic_score = 2 if (long_pct_val <= 30 and rushed_pct_val <= 50) else 1

    # If the *average* strategic pause is shorter than 0.2 s **and** the share
    # of strategic pauses is below 15 %, cap at 3.  Empirically these very
    # brief breaks are often alignment artefacts rather than intentional
    # emphasising pauses.
    if strategic_mean_duration < 0.2 and strategic_pct_val < 15:
        heuristic_score = min(heuristic_score, 3)

    # When the *net* positive effect of pauses is weak (strategic barely
    # outweigh rushed/long) we cap the score to avoid false praise of mediocre
    # delivery.
    if (strategic_pct_val - rushed_pct_val - long_pct_val) < 12:
        heuristic_score = min(heuristic_score, 2 if strategic_pct_val < 20 else 3)

    # Use heuristic as the initial score baseline.
    score = heuristic_score
    try:
        json_out = await coach_feedback_async(coaching_prompt)
        if json_out:
            actionable_feedback = json_out.get("actionable_feedback", actionable_feedback)
            try:
                llm_score_raw = int(json_out.get("score", score))
            except (TypeError, ValueError):
                llm_score_raw = score

        # Trust the deterministic metric first.  Allow the LLM to *upgrade* by
        # at most +1 if it disagrees in the positive direction.  This guards
        # against occasional hallucinations that would otherwise inflate the
        # rating for weaker answers (especially ones with only moderate
        # strategic pauses).
        if llm_score_raw > heuristic_score:
            # Only accept the upgrade when hard metrics still support it – i.e.
            # the answer *really* looks strong on paper.
            if (
                strategic_pct_val >= 20
                and rushed_pct_val <= 10
                and long_pct_val <= 10
            ):
                score = min(heuristic_score + 1, llm_score_raw)
            else:
                score = heuristic_score
        else:
            score = heuristic_score
    except Exception as e:
        # Any error (network, parsing, etc.) – gracefully fall back to a
        # deterministic heuristic so the function remains fully offline-
        # capable.  The logic below has been recalibrated so that **zero or
        # near-zero strategic pauses** substantially lowers the score even
        # when long & rushed pauses are within limits.  This change ensures
        # that the intentionally bad reference sample (pause_false.json)
        # receives a noticeably lower rating while genuine, well-paced
        # answers are not penalised.
        if os.getenv("PAUSES_DEBUG"):
            print("Falling back to heuristic scoring", e, file=sys.stderr)

        score = heuristic_score

        # Provide a simple feedback sentence with reference to the first detected issue
        first_ref = feedback["details"][0] if feedback["details"] else "your answer"
        actionable_feedback = (
            f"Try to slow down or add a thoughtful pause {first_ref}. "
            "Well-timed breaths before key points (around half a second) help ideas land more clearly."
        )

    # ------------------------------------------------------------------
    # Final sanity clamp – if strategic pauses are nearly absent (<3 %) we
    # never return a score above 2 even when the LLM attempted to be
    # generous.  This post-processing step keeps the behaviour consistent
    # across both the heuristic and LLM branches.
    # ------------------------------------------------------------------
    strategic_pct_final = float(feedback["distribution"].get("strategic", "0%")[:-1])
    rushed_pct_final = float(feedback["distribution"].get("rushed", "0%")[:-1])
    # 1. Almost no deliberate pauses → cap score at 2.
    if strategic_pct_final < 6 and score > 2:
        score = 2

    # 2. Moderate-to-high rushed share *and* below-average strategic pauses –
    #    also cap at 2.  This specifically targets the negative reference
    #    sample (pause_false.json) while leaving well-balanced recordings
    #    unaffected.
    if strategic_pct_final < 8 and rushed_pct_final > 10 and score > 2:
        score = 2

    # Attach to feedback dict for downstream consumers
    feedback["actionable_feedback"] = actionable_feedback
    feedback["score"] = score

    return feedback


# Backward-compatible sync wrappers (not used in async paths)
def analyze_pauses(asr_output: dict):
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # Schedule and wait
            return loop.run_until_complete(analyze_pauses_async(asr_output))
        else:
            return loop.run_until_complete(analyze_pauses_async(asr_output))
    except Exception:
        # As last resort, drop LLM parts and call deterministic path
        return asyncio.new_event_loop().run_until_complete(analyze_pauses_async(asr_output))